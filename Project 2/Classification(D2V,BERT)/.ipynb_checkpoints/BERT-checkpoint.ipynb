{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](LogoEPL.jpg)\n",
    "<b><p style='text-align: center;'> Algorithms in Data Science  </p> </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b><p style='text-align: center;'> Trump and Biden: Fans' tweet Classification: </p> </b>\n",
    "<b><p style='text-align: center;'> BERT </p> </b>\n",
    "\n",
    "\n",
    "<i><p style='text-align: Center;'> Nima Farnoodian , Charles Rongione, Breno Tiburcio</p> </i>\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing a pip package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install bert-for-tf2\n",
    "!{sys.executable} -m pip install sentencepiece\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "!{sys.executable} -m pip install tensorflow_hub\n",
    "!{sys.executable} -m pip install --upgrade tensorflow_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "Trump_train = pd.read_csv(\"C:/Users/b_tib/OneDrive - UCL/1F/oLINMA2472/Project/HW2/comments/The_Donald_train.csv\")\n",
    "Trump_train.insert(2, \"Candidate\", 0)\n",
    "Biden_train = pd.read_csv(\"C:/Users/b_tib/OneDrive - UCL/1F/oLINMA2472/Project/HW2/comments/JoeBiden_train.csv\")\n",
    "Biden_train.insert(2, \"Candidate\", 1)\n",
    "\n",
    "Trump_test = pd.read_csv(\"C:/Users/b_tib/OneDrive - UCL/1F/oLINMA2472/Project/HW2/comments/The_Donald_test.csv\")\n",
    "Trump_test.insert(2, \"Candidate\", 0)\n",
    "Biden_test = pd.read_csv(\"C:/Users/b_tib/OneDrive - UCL/1F/oLINMA2472/Project/HW2/comments/JoeBiden_test.csv\")\n",
    "Biden_test.insert(2, \"Candidate\", 1)\n",
    "\n",
    "train_set = pd.concat([Biden_train, Trump_train])\n",
    "test_set = pd.concat([Biden_test, Trump_test])\n",
    "\n",
    "# Spliting dataframe into interactive lists for body processing and then ensemble together again\n",
    "body_train=list(train_set['body'])\n",
    "body_test=list(test_set['body'])\n",
    "\n",
    "cand_train=list(train_set['Candidate'])\n",
    "cand_test=list(test_set['Candidate'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following script cleans all the comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)             # Remove punctuations and numbers\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)       # Single character removal\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)                  # Removing multiple spaces\n",
    "    return sentence\n",
    "\n",
    "def process_text(sentences):\n",
    "    processed_body=[]\n",
    "    for sen in sentences:\n",
    "        processed_body.append(preprocess_text(sen))\n",
    "    return processed_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You misspelled asshoe ', 'Reddit being cocksuckers as usual that what ', 'Yeah that what heard also But have to ask how is the secret service allowed to turn away someone with court orders signed by judge and notarized ', 'How many days has it been since this latest attempt at election interference Reddit has to be noticing the traffic decrease ', 'Look at the comment count vs the number of actual comments Enjoy your new communist overlords m sure reddit controlled moderation will be blast ', 'Look at the comment count vs the actual number of comments They are in the process of fine tuning automodderation to delete random comments It only matter of time before they unveil the new censorship in all its glory ', 'I got wanings including for upvoting wrongthink Am shadowbanned spez guess all were for upvoting wrongthink ', 'Less than half the comments are showing This is messed up ', 'You seem just little too content there champ', 'Yeah says but there like ']\n"
     ]
    }
   ],
   "source": [
    "body_train_processed=process_text(body_train)\n",
    "body_test_processed=process_text(body_test)\n",
    "print(body_train_processed[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "def nltk_ST(example_sent):\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "\n",
    "    word_tokens = word_tokenize(example_sent) \n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "\n",
    "    filtered_sentence = [] \n",
    "\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "            \n",
    "    filtered_sentence=' '.join(filtered_sentence) \n",
    "    \n",
    "    return filtered_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beto badass strikes', 'Imagine three together Look saying shot magical Democratic Party Past Present Future type moment three die simply die', 'Lmao fuck concern troll']\n"
     ]
    }
   ],
   "source": [
    "def body_ST(body_processed):\n",
    "\n",
    "    body_processed_ST=[]\n",
    "    for sen in body_processed:\n",
    "        sen_ST = nltk_ST(sen)\n",
    "        body_processed_ST.append(sen_ST)\n",
    "        \n",
    "    return body_processed_ST\n",
    "\n",
    "body_train_processed_ST=body_ST(body_train_processed)\n",
    "body_test_processed_ST=body_ST(body_test_processed)\n",
    "\n",
    "print(body_test_processed_ST[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Tolkenizer (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers### Creating Tokenizer\n",
    "\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer                                # create an object\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",      \n",
    "                            trainable=False)                                        # BERT model from hub.KerasLayer\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()          # create vocabulary np-array file   \n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()                    # set the text to lowercase\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)                           # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tolkenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing comments through the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_body(text):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def body_2_labels(bodies_processed, candidate):\n",
    "\n",
    "    tokenized_body = [tokenize_body(body) for body in bodies_processed]\n",
    "    \n",
    "    body_cand = [[comment, candidate[i], len(comment)]\n",
    "                 for i, comment in enumerate(tokenized_body)]  \n",
    "    \n",
    "    random.shuffle(body_cand)\n",
    "    comments_labels = [(comment_lab[0], comment_lab[1]) for comment_lab in body_cand]\n",
    "    \n",
    "    return comments_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = body_2_labels(body_train_processed_ST,cand_train)\n",
    "test_labels = body_2_labels(body_test_processed_ST,cand_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training __TensorFlow 2.0 models__:\n",
    "The batch size of 32 means the neural network will calibrate its weight after 32 comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_set = tf.data.Dataset.from_generator(lambda: train_labels, output_types=(tf.int32,tf.int32))\n",
    "processed_test_set = tf.data.Dataset.from_generator(lambda: test_labels, output_types=(tf.int32,tf.int32))\n",
    "\n",
    "\n",
    "BATCH_SIZE_TRAIN = 32 # 705 batches\n",
    "BATCH_SIZE_TEST = 32 # 6 batches\n",
    "batched_train_set = processed_train_set.padded_batch(BATCH_SIZE_TRAIN, padded_shapes=((None, ),()))\n",
    "batched_test_set = processed_test_set.padded_batch(BATCH_SIZE_TEST, padded_shapes=((None, ),()))\n",
    "\n",
    "TOTAL_BATCHES = math.ceil(len(train_labels) / BATCH_SIZE_TRAIN)\n",
    "\n",
    "batched_train_set.shuffle(TOTAL_BATCHES)\n",
    "\n",
    "train_data = batched_train_set\n",
    "test_data = batched_test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking how the first bacth look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 150), dtype=int32, numpy=\n",
       " array([[ 2002,  2064,  3828, ...,     0,     0,     0],\n",
       "        [ 1045,  2812,  4593, ...,     0,     0,     0],\n",
       "        [ 2017,  2757, 11504, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [10166,  4942,  4760, ...,     0,     0,     0],\n",
       "        [ 3929,  5993,  2057, ...,     0,     0,     0],\n",
       "        [ 2027,  2769,  9936, ...,     0,     0,     0]])>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "        1, 0, 0, 1, 1, 0, 1, 0, 1, 0])>)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model DCNN Model\n",
    "\n",
    "Our test model chosen consists on three convolutional hidden layers ranging from two to for kernels.\n",
    "In between each convolution, it applied Pool to reduce variance an computational (Maximum pooling in this case).  \n",
    "\n",
    "source: https://colab.research.google.com/drive/12noBxRkrZnIkHqvmdfFW2TGdOXFtNePM#scrollTo=6eh7sIquja5t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 emb_dim=128,\n",
    "                 nb_filters=50,\n",
    "                 FFN_units=512,\n",
    "                 nb_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"dcnn\"):\n",
    "        super(DCNN, self).__init__(name=name)\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size,\n",
    "                                          emb_dim)\n",
    "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
    "                                    kernel_size=2,\n",
    "                                    padding=\"valid\",\n",
    "                                    activation=\"relu\")\n",
    "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
    "                                     kernel_size=3,\n",
    "                                     padding=\"valid\",\n",
    "                                     activation=\"relu\")\n",
    "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
    "                                      kernel_size=4,\n",
    "                                      padding=\"valid\",\n",
    "                                      activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "\n",
    "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if nb_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=nb_classes,\n",
    "                                           activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.embedding(inputs)\n",
    "        x_1 = self.bigram(x) # batch_size, nb_filters, seq_len-1)\n",
    "        x_1 = self.pool(x_1) # (batch_size, nb_filters)\n",
    "        x_2 = self.trigram(x) # batch_size, nb_filters, seq_len-2)\n",
    "        x_2 = self.pool(x_2) # (batch_size, nb_filters)\n",
    "        x_3 = self.fourgram(x) # batch_size, nb_filters, seq_len-3)\n",
    "        x_3 = self.pool(x_3) # (batch_size, nb_filters)\n",
    "\n",
    "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 100 #200     # [200,1000] 50\n",
    "CNN_FILTERS = 32 # [32,64] 32\n",
    "DNN_UNITS = 512   # dense layer  [32,512]\n",
    "DROPOUT_RATE = 0.20 # for each layer [0.1,0.4]\n",
    "NB_EPOCHS = 1     # times [3,10]\n",
    "OUTPUT_CLASSES = 2\n",
    "\n",
    "text_model = DCNN(vocab_size=VOCAB_LENGTH,\n",
    "                        emb_dim=EMB_DIM,\n",
    "                        nb_filters=CNN_FILTERS,\n",
    "                        FFN_units=DNN_UNITS,\n",
    "                        nb_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706/706 [==============================] - 52s 73ms/step - loss: 0.3970 - accuracy: 0.8053 - val_loss: 0.3392 - val_accuracy: 0.8402\n",
      "177/177 [==============================] - 2s 11ms/step - loss: 0.3392 - accuracy: 0.8402\n",
      "[0.3392384946346283, 0.8401842713356018]\n"
     ]
    }
   ],
   "source": [
    "# Compiling Model\n",
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(loss=\"binary_crossentropy\", #log_cosh\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"accuracy\"])\n",
    "else:\n",
    "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"sparse_categorical_accuracy\"])\n",
    "    \n",
    "#Fitting Model\n",
    "text_model.fit(train_data, epochs = NB_EPOCHS, validation_data = test_data)\n",
    "\n",
    "\n",
    "#TEsting Model\n",
    "results = text_model.evaluate(test_data)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
